Explain the role of attention masks in transformer models and how they affect decoding.

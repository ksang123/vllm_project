# Minimal benchmark configuration focused on performance.

log_config:
  log_file: "benchmark.log"

server_config:
  model: "Qwen/Qwen3-14B"
  api_key: "EMPTY"
  host: "127.0.0.1"
  port: 8000
  base_url: null  # auto-derived from host/port if null
  max_model_len: 32768
  gpu_memory_utilization: 0.9
  dtype: "auto"
  tensor_parallel_size: 2
  quantization: null
  enable_prefix_caching: true
  tokenizer_mode: "auto"
  trust_remote_code: false
  engine_use_ray: false
  disable_log_stats: false
  disable_log_requests: false
  max_num_seqs: 256
  max_num_batched_tokens: 4096
  block_size: 16
  swap_space: 4
  cpu_offload_gb: 0

benchmark_config:
  dataset_name: "random"
  prompts_dir: "./prompts"
  num_prompts: 200
  warmup_prompts: 25
  client_processes: 32
  max_concurrency: 32
  target_rps: "inf"   # overall target requests per second ("inf" for max)
  random_input_len: 512
  random_output_len: 128
  temperature: 0.7
  top_p: 0.95
  seed: 42
  n: 1
  presence_penalty: 0.0
  frequency_penalty: 0.0
  stop: []
  stream: false
  logprobs: null
  api_base: null
  debug: false

output_config:
  output_dir: "./benchmark_results"
  save_json: true
  save_charts: true
  show_stats: true

telemetry_config:
  enable_gpu: true
  enable_cpu: true
  sample_interval_s: 1.0
